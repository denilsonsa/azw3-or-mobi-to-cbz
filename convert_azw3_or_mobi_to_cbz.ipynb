{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbccb341-7d57-48c5-ac23-629ca06dbead",
   "metadata": {},
   "source": [
    "# azw3-or-mobi-to-cbz\n",
    "\n",
    "Please read the [README](README.md) file for an introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d4bce-1082-485f-ba9c-6ed2c94f9e0b",
   "metadata": {},
   "source": [
    "This notebook is split in a few sections:\n",
    "\n",
    "* **Library imports and utility functions**\n",
    "    * If you have trouble with the dependencies, take a look here.\n",
    "    * In normal cases, you don't need to look here, because `pip install -r requirements.txt` is usually enough.\n",
    "* **MOBI-TO-CBZ parsing and conversion code**\n",
    "    * The main logic of this code. It extracts data from the e-book, does a bunch of sanity checks, and writes a CBZ file.\n",
    "    * In normal cases, you don't need to look here.\n",
    "    * If you want to understand what this code does, or if you want to debug why this code is not working with your e-book; then this is the place to look.\n",
    "* **Running it against your files**\n",
    "    * This is where you have to edit this notebook, to adapt to your files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86647a5-3639-4ccf-aed9-c528f7a167fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Library imports and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a579cacb-9ce7-4105-8d04-262d532a9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://beautiful-soup-4.readthedocs.io/\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fe6f4-2475-4870-80da-1465e0428cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/iscc/mobi/blob/master/README.md\n",
    "import mobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef38771-abc0-48e5-bb7f-2bc21934a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lxml.de/tutorial.html#the-e-factory\n",
    "from lxml import etree\n",
    "from lxml.builder import ElementMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8864d380-8ffd-48b5-9cdb-7d760011db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tqdm.github.io/\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6be674-c94b-4d15-9389-3fa574d7d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import shutil\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from datetime import date\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7dab4-4ab2-48bb-a982-c27f28c3ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/7961363/removing-duplicates-in-lists\n",
    "# Returns a list filtering out any duplicate items.\n",
    "# If you don't care about the order, don't use this function. Instead, just use the built-in set() type.\n",
    "def uniq(sequence):\n",
    "    return list(dict.fromkeys(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1a3ce-0599-4fd2-a54b-ce8552791887",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MOBI-to-CBZ parsing and conversion code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214c07d-336d-416f-a98c-a245ebc8dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/anansi-project/comicinfo\n",
    "# This function reads the `…/mobi8/OEBPS/content.opf` metadata file\n",
    "# and returns a nice `comicinfo.xml` output.\n",
    "# It also returns whether this book is fixed layout (pre-paginated).\n",
    "# It also returns the cover image filename.\n",
    "def comicinfo_converter(content_opf_filename: Path):\n",
    "    with open(content_opf_filename) as f:\n",
    "        content_opf = f.read().replace('<!-- BEGIN INFORMATION ONLY', '').replace('END INFORMATION ONLY -->', '')\n",
    "    doc = BeautifulSoup(content_opf, 'xml')\n",
    "    metadata = doc.metadata\n",
    "\n",
    "    def metatext(*args, **kwargs):\n",
    "        if el := metadata.find(*args, **kwargs):\n",
    "            if el.name == 'meta' and 'content' in el.attrs:\n",
    "                # Used for `<meta name=\"…\" content=\"…\" />` tags\n",
    "                return el['content']\n",
    "            else:\n",
    "                # Used for `<meta property=\"…\">…</meta>` tags\n",
    "                # Used for `<dc:foobar>…</dc:foobar` tags\n",
    "                return el.string.strip()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    datestr = metatext('dc:date')\n",
    "    dateobj = None\n",
    "    if datestr:\n",
    "        try:\n",
    "            date.fromisoformat(datestr)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    fixed_layout = metatext('meta', attrs={'name': 'fixed-layout'})\n",
    "    rendition_layout = metatext('meta', property='rendition:layout')\n",
    "\n",
    "    E = ElementMaker()\n",
    "    comicinfo = E.ComicInfo(\n",
    "        # Either this title, or the one from <meta name=\"Updated_Title\" content=\"…\" />\n",
    "        E.Title(metatext('dc:title') or ''),\n",
    "        E.Summary(metatext('dc:description') or ''),\n",
    "        *(\n",
    "            [\n",
    "                E.Year(str(dateobj.year)),\n",
    "                E.Month(str(dateobj.month)),\n",
    "                E.Day(str(dateobj.day)),\n",
    "            ] if dateobj else []\n",
    "        ),\n",
    "        E.Publisher(metatext('dc:publisher') or ''),\n",
    "        E.LanguageISO(metatext('dc:language') or ''),\n",
    "\n",
    "        # Mapping \"Creator\" to \"Writer\", and hoping it's a good enough match.\n",
    "        E.Writer(metatext('dc:creator') or ''),\n",
    "        # E.Penciller(),\n",
    "        # E.Inker(),\n",
    "        # E.Colorist(),\n",
    "        # E.Letterer(),\n",
    "        # E.CoverArtist(),\n",
    "        # E.Editor(),\n",
    "\n",
    "        # E.Series(),\n",
    "        # E.Number(),\n",
    "        # E.Count(),\n",
    "        # E.Volume(),\n",
    "        # E.AlternateSeries(),\n",
    "        # E.AlternateNumber(),\n",
    "        # E.AlternateCount(),\n",
    "        # E.Notes(),\n",
    "        # E.Imprint(),\n",
    "        # E.Genre(),\n",
    "        # E.Web(),\n",
    "        # E.PageCount(),\n",
    "        # E.Format(),\n",
    "        # E.BlackAndWhite(),\n",
    "        # E.Manga(),\n",
    "        # E.Characters(),\n",
    "        # E.Teams(),\n",
    "        # E.Locations(),\n",
    "        # E.ScanInformation(),\n",
    "        # E.StoryArc(),\n",
    "        # E.SeriesGroup(),\n",
    "        # E.AgeRating(),\n",
    "        # E.MainCharacterOrTeam(),\n",
    "        # E.Review(),\n",
    "    )\n",
    "\n",
    "    guide = doc.guide\n",
    "    # There is also type=\"other.ms-coverimage\", which is ignored here.\n",
    "    # There is also type=\"other.ms-coverimage-standard\", which is ignored here.\n",
    "    ref = guide.find('reference', type='cover')\n",
    "    cover = ref.href if ref else None\n",
    "    # Even though we try to extract the cover here, it ends up unused in the rest of the code.\n",
    "\n",
    "    return (\n",
    "        etree.tostring(comicinfo, xml_declaration=True, encoding='utf-8'),\n",
    "        fixed_layout,\n",
    "        rendition_layout,\n",
    "        cover,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49c92ba-22fa-4cbb-a4b3-c29cdd025a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_from_css(csspath: Path):\n",
    "    try:\n",
    "        text = csspath.read_text()\n",
    "        text = re.sub(r'@font-face\\s*{[^}]*}', '', text)\n",
    "        matches = re.findall(r'''url\\(['\"]?([^)]+)['\"]?\\)''', text)\n",
    "        return [csspath.parent / url for url in matches if url != 'XXXXXXXXXXXXXXXX']\n",
    "    except FileNotFoundError as e:\n",
    "        if csspath.name == 'XXXXXXXXXXXXXXXX':\n",
    "            # Ignore bogus href.\n",
    "            return []\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded615a-a27d-467e-86a4-d8c6ebde14d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_from_svg(svg_element, basedir: Path):\n",
    "    for image in svg_element.find_all('image'):\n",
    "        href = image['xlink:href']\n",
    "        if href:\n",
    "            yield basedir / href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48bb99-53f3-450a-8ad6-2064cc656f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_single_page_from_extracted_mobi(fname, basedir):\n",
    "    images = []\n",
    "    messages = []\n",
    "\n",
    "    with open(fname) as f:\n",
    "        doc = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "    # Capturing images from this page:\n",
    "    # Images linked from the CSS stylesheet.\n",
    "    uniq_css_href = uniq(link_element['href'] for link_element in doc.find_all('link', rel='stylesheet'))\n",
    "    from_css = uniq(chain.from_iterable(extract_urls_from_css(fname.parent / href) for href in uniq_css_href if href))\n",
    "    # if from_css:\n",
    "    #     pass\n",
    "    #     # print(bookpath.name, fname.relative_to(basedir), 'link stylesheets pointing to:', from_css)\n",
    "    # Images linked from an inline SVG element. Usually for the book cover.\n",
    "    from_svg = uniq(chain.from_iterable(extract_urls_from_svg(svg, fname.parent) for svg in doc.body.find_all('svg')))\n",
    "    # if from_svg:\n",
    "    #     pass\n",
    "    #     # print(bookpath.name, fname.relative_to(basedir), 'svg pointing to:', from_svg)\n",
    "    # Images included via the good old `<img src=\"…\" />` tag.\n",
    "    from_img = uniq(fname.parent / img['src'] for img in doc.body.find_all('img'))\n",
    "    # if from_img:\n",
    "    #     pass\n",
    "    #     # print(bookpath.name, fname.relative_to(basedir), 'img pointing to:', from_img)\n",
    "    images = uniq(chain(from_css, from_svg, from_img))\n",
    "    if len(images) == 2:\n",
    "        # Doing some sanity checks.\n",
    "        left = doc.select('.leftPage#page-img-left')\n",
    "        right = doc.select('.rightPage#page-img-right')\n",
    "        idGen = doc.select('div[id^=\"_idContainer\"] > img._idGenObjectAttribute-1._idGenObjectAttribute-2')\n",
    "        if len(left) == len(right) == 1:\n",
    "            # Easy and straigh-forward markup.\n",
    "            pass\n",
    "        elif len(idGen) == 2:\n",
    "            # This is a more convoluted markup, but also easy enough.\n",
    "            pass\n",
    "        else:\n",
    "            # This is a different markup than everything else.\n",
    "            messages.append(\n",
    "                'WARNING page {} has two images in a single page, but using some unique markup.\\n{}'.format(\n",
    "                    fname.relative_to(basedir),\n",
    "                    '\\n'.join(str(p) for p in images),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # This is a two-page per XHTML document.\n",
    "        # Let's just assume the images are listed in the correct order in the CSS.\n",
    "        # For the few books I manually inspected, this assumption is correct.\n",
    "        # This logic will fail if the CSS lists the right page before the left one.\n",
    "        if images != sorted(images):\n",
    "            messages.append(\n",
    "                'WARNING page {} has two images in this page, but in reverse order.\\n{}'.format(\n",
    "                    fname.relative_to(basedir),\n",
    "                    '\\n'.join(str(p) for p in images),\n",
    "                )\n",
    "            )\n",
    "    elif len(images) > 2:\n",
    "        messages.append(\n",
    "            'ERROR too many images in a single page: {} has {} images.'.format(\n",
    "                fname.relative_to(basedir),\n",
    "                len(images),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Checking for text inside the page.\n",
    "    body = doc.body\n",
    "    text = ' '.join(body.stripped_strings)\n",
    "    if text == \"\":\n",
    "        if len(images) == 0:\n",
    "            messages.append(\n",
    "                'WARNING page {} has no text and no images.'.format(fname.relative_to(basedir))\n",
    "            )\n",
    "    else:\n",
    "        # Some books have \"invisible\" text overlaid on top of the image.\n",
    "        # Those are usuallly clickable links.\n",
    "        messages.append('TEXT {} {}'.format(fname.relative_to(basedir), text))\n",
    "\n",
    "    return images, messages, len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8830182-b868-43fa-b584-dc6f31016ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BookResults:\n",
    "    path: str\n",
    "    cbzpath: str = ''\n",
    "    converted: bool = False\n",
    "    page_count: int = 0\n",
    "    image_count: int = 0\n",
    "    char_count: int = 0\n",
    "    max_chars_per_page: int = 0\n",
    "    errors: int = 0\n",
    "    warnings: int = 0\n",
    "    messages: list[str] = field(default_factory=list)\n",
    "\n",
    "    # Why having this method instead of having `.errors` and `.warnings` as getters?\n",
    "    # Because only dataclass fields get converted to key/value in `asdict()` function.\n",
    "    def count_messages(self):\n",
    "        self.errors = 0\n",
    "        self.warnings = 0\n",
    "        for m in self.messages:\n",
    "            if m.startswith('ERR'):\n",
    "                self.errors += 1\n",
    "            elif m.startswith('WARN'):\n",
    "                self.warnings += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f7d86-f636-4e39-9142-7687a02b7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mobi_to_cbz(\n",
    "    bookpath: Path,\n",
    "    *,\n",
    "    keep_tempdir_because_i_am_debugging: bool = False,\n",
    "    exclude_images: set = None,\n",
    "    exclude_duplicates: bool = True,\n",
    ") -> BookResults:\n",
    "    '''Given a MOBI or AZW3 filepath, writes a CBZ file and returns a BookResults instance.\n",
    "\n",
    "    Parameters:\n",
    "    bookpath - Must be a Path instance. If you have a string, please convert it to Path before passing to this function.\n",
    "    keep_tempdir_because_i_am_debugging - Boolean. Enable it to keep the temporary directory (instead of automatically deleting it).\n",
    "    exclude_images - Set. These image names will be excluded from the CBZ.\n",
    "    exclude_duplicates - Boolean. Don't add duplicate images to the CBZ.\n",
    "    '''\n",
    "    ret = BookResults(path=str(bookpath))\n",
    "    try:\n",
    "        tempdir, filepath = mobi.extract(str(bookpath))\n",
    "    except Exception as e:\n",
    "        ret.messages.append('ERROR cannot extract the book: {}'.format(e))\n",
    "        ret.count_messages()\n",
    "        return ret\n",
    "        \n",
    "    tempdir = Path(tempdir)\n",
    "    # Verbose progress output:\n",
    "    print('Processing {!r} using temp directory {!r}'.format(bookpath, tempdir))\n",
    "\n",
    "    try:\n",
    "        comicinfo_xml, fixed_layout, rendition_layout, cover = comicinfo_converter(tempdir / 'mobi8/OEBPS/content.opf')\n",
    "        if fixed_layout is None and rendition_layout is None:\n",
    "            # Non-fixed layout ebook. This isn't a comic book. Nothing to do here.\n",
    "            ret.messages.append('SKIPPED non-fixed layout, this is not a comic book.')\n",
    "            ret.count_messages()\n",
    "            return ret\n",
    "        elif fixed_layout == 'false' and rendition_layout == 'reflowable':\n",
    "            # Non-fixed layout ebook. This isn't a comic book. Nothing to do here.\n",
    "            ret.messages.append('SKIPPED non-fixed layout, this is not a comic book.')\n",
    "            ret.count_messages()\n",
    "            return ret\n",
    "        elif fixed_layout == 'true' and rendition_layout == 'pre-paginated':\n",
    "            pass\n",
    "        else:\n",
    "            # Sanity check.\n",
    "            ret.messages.append('ERROR unsupported values for fixed_layout={!r} and rendition_layout={!r}.'.format(fixed_layout, rendition_layout))\n",
    "            ret.count_messages()\n",
    "            return ret\n",
    "\n",
    "        all_images = []\n",
    "\n",
    "        # The filenames are:\n",
    "        # * cover_page.xhtml\n",
    "        # * nav.xhtml\n",
    "        # * part0000.xhtml … part9999.xhtml\n",
    "        for fname in sorted(tempdir.glob('**/*.xhtml')):\n",
    "            if fname.name == 'nav.xhtml':\n",
    "                continue\n",
    "\n",
    "            page_images, page_messages, page_chars = parse_single_page_from_extracted_mobi(fname, tempdir)\n",
    "\n",
    "            all_images.extend(page_images)\n",
    "            ret.page_count += 1\n",
    "            ret.messages.extend(page_messages)\n",
    "            ret.char_count += page_chars\n",
    "            ret.max_chars_per_page = max(ret.max_chars_per_page, page_chars)\n",
    "\n",
    "        # Removing files that cannot be found.\n",
    "        # Likely due to broken CSS. (e.g. non-existing background image)\n",
    "        # Also removing images explicitly excluded by the caller.\n",
    "        # Also resolving the paths, removing the `..` parts. This is needed in order to detect duplicates.\n",
    "        tmp = []\n",
    "        for img in all_images:\n",
    "            if exclude_images and img.name in exclude_images:\n",
    "                pass\n",
    "            elif not img.is_file():\n",
    "                ret.messages.append('WARNING image not found {}'.format(img))\n",
    "            else:\n",
    "                tmp.append(img.resolve())\n",
    "        all_images = tmp\n",
    "\n",
    "        ret.image_count = len(all_images)\n",
    "        if ret.image_count == 0:\n",
    "            ret.messages.append('SKIPPED no images found in this book.')\n",
    "            ret.count_messages()\n",
    "            return ret\n",
    "\n",
    "        # Checking for duplicate images across multiple pages.\n",
    "        if len(all_images) != len(set(all_images)):\n",
    "            cnt = Counter(all_images)\n",
    "            if exclude_duplicates:\n",
    "                ret.messages.append('WARNING duplicate images across pages.')\n",
    "                all_images = uniq(all_images)\n",
    "            else:\n",
    "                ret.messages.append('ERROR duplicate images across pages.')\n",
    "            ret.messages.extend(['DUPLICATE {}x {}'.format(v, k) for k, v in cnt.items() if v > 1])\n",
    "\n",
    "        if all_images != sorted(all_images):\n",
    "            ret.messages.append('WARNING image filenames are not in the alphabetical order')\n",
    "\n",
    "        # All sanity checks passed.\n",
    "\n",
    "        ret.cbzpath = str(bookpath.with_suffix('.cbz'))\n",
    "        with ZipFile(ret.cbzpath, 'w') as cbz:\n",
    "            cbz.writestr('comicinfo.xml', comicinfo_xml)\n",
    "            for i, img in enumerate(all_images):\n",
    "                ext = img.suffix.replace('jpeg', 'jpg')\n",
    "                cbz.write(img, 'page{:03}{}'.format(i + 1, ext))\n",
    "\n",
    "        ret.converted = True\n",
    "        ret.count_messages()\n",
    "        return ret\n",
    "\n",
    "    finally:\n",
    "        if not keep_tempdir_because_i_am_debugging:\n",
    "            shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97cde5f-d06a-41d4-a9b9-2114a435dafb",
   "metadata": {},
   "source": [
    "## Running it against your files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc26ae-7a7b-4d09-a5ce-48909af5abc1",
   "metadata": {},
   "source": [
    "You have several ways to run this code.\n",
    "\n",
    "Let's start by some examples on how to run it for a single file. Let's say that file is called `BOOK`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b362c-5e82-4161-a844-ccc099e44d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOK = Path(\"/home/foobar/Books/Example Comic Book.azw3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b574a52d-e21d-462d-984f-70edeb79a803",
   "metadata": {},
   "source": [
    "Note: Please ignore this message:\n",
    "\n",
    "> Warning: Bad key, size, value combination detected in EXTH  406 16 0000000000000000\n",
    "\n",
    "This warning is coming from the `mobi` library, and it is harmless. Unfortunately, there is no option to stop printing this warning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca2182a-fe60-4d1a-afa1-461675add357",
   "metadata": {},
   "source": [
    "You can convert it to CBZ using the default options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0913b-220e-4af9-bbf3-1833f2a10a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = convert_mobi_to_cbz(BOOK)\n",
    "print(json.dumps(asdict(out), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a368e44-dc44-4a54-82ae-9310cdc10877",
   "metadata": {},
   "source": [
    "For most cases, you probably want to skip duplicate images.\n",
    "\n",
    "(Because the code here is very simplified and takes shortcuts, it can detect the same image across multiple pages, even if that image isn't shown on such pages. Learn more about how the code works by reading the README and also reading the actual code.)\n",
    "\n",
    "Still, for in some cases you may want to keep duplicate images for your specific book, and you can do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ead23-b9d9-4631-94e6-54d88a4c1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = convert_mobi_to_cbz(BOOK, exclude_duplicates=False)\n",
    "print(json.dumps(asdict(out), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29455ee4-3245-4ce2-a8b0-83a7f0d90c54",
   "metadata": {},
   "source": [
    "In some cases, there may be some decorative images that are being incorrectly added to the CBZ. You can easily exclude them, in a case-by-case manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d43b64-17c1-4c4e-831b-c2cabddaea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = convert_mobi_to_cbz(BOOK, exclude_images={'image00015.gif', 'image00023.jpeg'})\n",
    "print(json.dumps(asdict(out), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193054f-f6fb-46f1-a00f-6e8856366b6b",
   "metadata": {},
   "source": [
    "Sometimes you need to debug what is going on. That's also easy to do, just remember to manually delete the temporary directory after you're finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faedfab-8869-49d7-88cc-35a18930753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = convert_mobi_to_cbz(BOOK, keep_tempdir_because_i_am_debugging=True)\n",
    "print(json.dumps(asdict(out), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439fdc5-8793-4763-aa90-5a4c80f0960e",
   "metadata": {},
   "source": [
    "Finally, you may have a directory full of e-books. You may want to convert them all to CBZ. Well, not all of them, but just those that have a fixed layout. And you may also want to save some statistics/diagnostics log as a JSON file for later inspection.\n",
    "\n",
    "This is also easy to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2429a4-e81a-4594-b118-049d3f6ee7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEDIR = Path('/home/foobar/Books/')\n",
    "with open(BASEDIR / '_MOBI_TO_CBZ_LOG.json', 'w') as logfile:\n",
    "    json.dump([\n",
    "        asdict(convert_mobi_to_cbz(book))\n",
    "        for book in tqdm(sorted(BASEDIR.glob('*.azw*')))\n",
    "    ], logfile, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
